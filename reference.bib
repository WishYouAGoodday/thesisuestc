@Book{Deeplearning,
  	Title                    = {Deep Learning},
 	 Author                   = {Ian J. Goodfellow and Yoshua Bengio and Aaron Courville},
 	 Publisher                = {MIT Press},
 	 Year                     = {2016},
	  Address                  = {Cambridge, MA, USA},
	  Note                     = {\url{http://www.deeplearningbook.org}}
}

@inproceedings{Transformer,
 	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
	 booktitle = {Advances in Neural Information Processing Systems},
	 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	 pages = {},
	 publisher = {Curran Associates, Inc.},
 	title = {Attention is All you Need},
	 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	 volume = {30},
	 year = {2017}
}
@inproceedings{GPT1,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}
@article{GPT2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}
@misc{GPT4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@ARTICLE{CDSC
  author={Cong, Jason and Sarkar, Vivek and Reinman, Glenn and Bui, Alex},
  journal={IEEE Design /& Test of Computers}, 
  title={Customizable Domain-Specific Computing}, 
  year={2011},
  volume={28},
  number={2},
  pages={6-15},
  doi={10.1109/MDT.2010.141}
}
@article{DSA,
  title={A new golden age for computer architecture},
  author={Hennessy, John L and Patterson, David A},
  journal={Communications of the ACM},
  volume={62},
  number={2},
  pages={48--60},
  year={2019},
  publisher={ACM New York, NY, USA}
}
@article{DianNao,
	author = {Chen, Yunji and Chen, Tianshi and Xu, Zhiwei and Sun, Ninghui and Temam, Olivier},
	title = {DianNao Family: Energy-Efficient Hardware Accelerators for Machine Learning},
	year = {2016},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {59},
	number = {11},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/2996864},
	doi = {10.1145/2996864},
	journal = {Commun. ACM},
	month = {oct},
	pages = {105–112},
	numpages = {8}
}
@inproceedings{FPGA_ACC,
	author = {Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason},
	title = {Optimizing FPGA-Based Accelerator Design for Deep Convolutional Neural Networks},
	year = {2015},
	isbn = {9781450333153},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2684746.2689060},
	doi = {10.1145/2684746.2689060},
	booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	pages = {161–170},
	numpages = {10},
	keywords = {fpga, acceleration, roofline model, convolutional neural network},
	location = {Monterey, California, USA},
	series = {FPGA '15}
}

@article{Hornik,
  title = {Multilayer Feedforward Networks are Universal Approximators},
  author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
  journal = {IEEE Transactions on Neural Networks},
  volume = {2},
  number = {5},
  pages = {359 -- 366},
  year = {1989}
}

@book{ComputerArchi,
  title = {Computer Architecture: A Quantitative Approach},
  author = {John Hennessy and David Patterson},
  year = {2017},
  address = {America},
  pages = {1-2},
  publisher = {Elsevier}
}


@article{Bengio1994b,
  title = {Learning long-term dependencies with gradient descent is difficult},
  author = {Y. Bengio and P. Simard and P. Frasconi},
  journal = {IEEE Transactions on Neural Networks},
  volume = {5},
  number = {2},
  pages = {157 -- 166},
  year = {1994}
}

@article{Hochreiter,
	author = {Hochreiter, Sepp},
	journal = {docter degree},
	year = {1991},
	month = {04},
	pages = {},
	title = {Untersuchungen zu dynamischen neuronalen Netzen}
}



@article{NAS_ICLR,
	author = {Zoph, Barret and Le, Quoc},
	journal = {Proc. ICLR},
	year = {2017},
	month = {4},
	pages = {1-2},
	title = {Neural Architecture Search with Reinforcement Learning}
}

@conference{NAS_CVPR,
  author={Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Learning Transferable Architectures for Scalable Image Recognition}, 
  year={2018},
  volume={},
  number={},
  pages={8697-8710},
  doi={10.1109/CVPR.2018.00907}
}

@article{MobileNetsV1,
  author    = {Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko and Weijun Wang},
  title     = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  journal   = {CoRR},
  volume    = {abs/1704.04861},
  year      = {2017}
}

@article{MobileNetsV2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4510--4520},
  year={2018}
}
@inproceedings{MobileNetsV2,
  author    = {Mark Sandler and
               Andrew G. Howard and
               Menglong Zhu and
               Andrey Zhmoginov and
               Liang{-}Chieh Chen},
  title     = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
  booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages     = {4510--4520},
  publisher = {Computer Vision Foundation / {IEEE} Computer Society},
  year      = {2018}
}


@misc{squeezenet,
	title={SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and \ensuremath{<}0.5{MB} model size},
	author={Forrest N. Iandola and Song Han and Matthew W. Moskewicz and Khalid Ashraf and William J. Dally and Kurt Keutzer},
	year={2017},
	url={https://openreview.net/forum?id=S1xh5sYgx}
}

@conference{Shuffnet,
  author={Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices}, 
  year={2018},
  volume={},
  number={},
  pages={6848-6856},
  doi={10.1109/CVPR.2018.00716}
}

@conference{CompRNN,
	author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
	title = {An Empirical Exploration of Recurrent Network Architectures},
	year = {2015},
	publisher = {JMLR.org},
	booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
	pages = {2342–2350},
	location = {Lille, France},
}

@inproceedings{GRU,
  author    = {Kyunghyun Cho and
               Bart van Merrienboer and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Dzmitry Bahdanau and
               Fethi Bougares and
               Holger Schwenk and
               Yoshua Bengio},
  editor    = {Alessandro Moschitti and
               Bo Pang and
               Walter Daelemans},
  title     = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical
               Machine Translation},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2014, October 25-29, 2014, Doha, Qatar,
               {A} meeting of SIGDAT, a Special Interest Group of the {ACL}},
  pages     = {1724--1734},
  publisher = {{ACL}},
  year      = {2014},
  url       = {https://doi.org/10.3115/v1/d14-1179},
  doi       = {10.3115/v1/d14-1179}
}

@article{slstm,
	title={Sentence-State LSTM for Text Representation},
	author={Zhang, Yue and Liu, Qi and Song, Linfeng},
	booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL)}, year={2018}
}
@article{mgu,
	title = {Minimal Gated Unit for Recurrent Neural Networks},
	journal = {International Journal of Automation and Computing},
	volume = {13},
	number = {gjzdhyjszz-13-03-226},
	pages = {226},
	year = {2016},
	issn = {2731-538X},
	doi = {10.1007/s11633-016-1006-2},
	url = {https://www.mi-research.net/en/article/doi/10.1007/s11633-016-1006-2},
	author = {Guo-Bing Zhou,Jianxin Wu,Chen-Lin Zhang,Zhi-Hua Zhou}
}
@article{JANET,
  title={The unreasonable effectiveness of the forget gate},
  author={Van Der Westhuizen, Jos and Lasenby, Joan},
  journal={arXiv preprint arXiv:1804.04849},
  year={2018}
}


@inproceedings{purning_filters,
	title={Pruning Filters for Efficient ConvNets},
	author={Hao, Li and Asim, Kadav and Igor, Durdanovic and Hanan, Samet and Hans, Peter},
	booktitle={International Conference on Learning Representations},
	year={2017},
	url={https://openreview.net/forum?id=rJqFGTslg}
}

@InProceedings{purning_channel,
	author = {He, Yihui and Zhang, Xiangyu and Sun, Jian},
	title = {Channel Pruning for Accelerating Very Deep Neural Networks},
	booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
	month = {Oct},
	year = {2017}
}
@inproceedings{WeightPurning,
 author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 title = {Learning both Weights and Connections for Efficient Neural Network},
 url = {https://proceedings.neurips.cc/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf},
 volume = {28},
 year = {2015}
}

@ARTICLE{Purning,
  author={Karnin, E.D.},
  journal={IEEE Transactions on Neural Networks}, 
  title={A simple procedure for pruning back-propagation trained neural networks}, 
  year={1990},
  volume={1},
  number={2},
  pages={239-242},
  doi={10.1109/72.80236}
}

@inproceedings{ESE,
	author = {Han, Song and Kang, Junlong and Mao, Huizi and Hu, Yiming and Li, Xin and Li, Yubin and Xie, Dongliang and Luo, Hong and Yao, Song and Wang, Yu and Yang, Huazhong and Dally, William (Bill) J.},
	title = {ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA},
	year = {2017},
	isbn = {9781450343541},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3020078.3021745},
	doi = {10.1145/3020078.3021745},
	booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	pages = {75–84},
	location = {Monterey, California, USA},
	series = {FPGA '17}
}

@inproceedings{C-LSTM,
	author = {Wang, Shuo and Li, Zhe and Ding, Caiwen and Yuan, Bo and Qiu, Qinru and Wang, Yanzhi and Liang, Yun},
	title = {C-LSTM: Enabling Efficient LSTM Using Structured Compression Techniques on FPGAs},
	year = {2018},
	isbn = {9781450356145},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3174243.3174253},
	doi = {10.1145/3174243.3174253},
	booktitle = {Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	pages = {11–20},
	location = {Monterey, CALIFORNIA, USA},
	series = {FPGA '18}
}

@inproceedings{Cao,
	author = {Cao, Shijie and Zhang, Chen and Yao, Zhuliang and Xiao, Wencong and Nie, Lanshun and Zhan, Dechen and Liu, Yunxin and Wu, Ming and Zhang, Lintao},
	title = {Efficient and Effective Sparse LSTM on FPGA with Bank-Balanced Sparsity},
	year = {2019},
	isbn = {9781450361378},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3289602.3293898},
	doi = {10.1145/3289602.3293898},
	abstract = {Neural networks based on Long Short-Term Memory (LSTM) are widely deployed in latency-sensitive language and speech applications. To speed up LSTM inference, previous research proposes weight pruning techniques to reduce computational cost. Unfortunately, irregular computation and memory accesses in unrestricted sparse LSTM limit the realizable parallelism, especially when implemented on FPGA. To address this issue, some researchers propose block-based sparsity patterns to increase the regularity of sparse weight matrices, but these approaches suffer from deteriorated prediction accuracy. This work presents Bank-Balanced Sparsity (BBS), a novel sparsity pattern that can maintain model accuracy at a high sparsity level while still enable an efficient FPGA implementation. BBS partitions each weight matrix row into banks for parallel computing, while adopts fine-grained pruning inside each bank to maintain model accuracy. We develop a 3-step software-hardware co-optimization approach to apply BBS in real FPGA hardware. First, we propose a bank-balanced pruning method to induce the BBS pattern on weight matrices. Then we introduce a decoding-free sparse matrix format, Compressed Sparse Banks (CSB), that transparently exposes inter-bank parallelism in BBS to hardware. Finally, we design an FPGA accelerator that takes advantage of BBS to eliminate irregular computation and memory accesses. Implemented on Intel Arria-10 FPGA, the BBS accelerator can achieve 750.9 GOPs on sparse LSTM networks with a batch size of 1. Compared to state-of-the-art FPGA accelerators for LSTM with different compression techniques, the BBS accelerator achieves 2.3 ~ 3.7x improvement on energy efficiency and 7.0 ~ 34.4x reduction on latency with negligible loss of model accuracy.},
	booktitle = {Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	pages = {63–72},
	location = {Seaside, CA, USA},
	series = {FPGA '19}
}

@article{AutoMl1,
	title = {AutoPruner: An end-to-end trainable filter pruning method for efficient deep model inference},
	journal = {Pattern Recognition},
	volume = {107},
	pages = {107461},
	year = {2020},
	issn = {0031-3203},
	doi = {https://doi.org/10.1016/j.patcog.2020.107461},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320302648},
	author = {Jian-Hao Luo and Jianxin Wu}
}

@inproceedings{AutoMl2,
	author = {He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
	title = {AMC: AutoML for Model Compression and Acceleration on Mobile Devices},
	year = {2018},
	isbn = {978-3-030-01233-5},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	url = {https://doi.org/10.1007/978-3-030-01234-2_48},
	doi = {10.1007/978-3-030-01234-2_48},
	booktitle = {Computer Vision – ECCV 2018: 15th European Conference, Munich, Germany, September 8–14, 2018, Proceedings, Part VII},
	pages = {815–832},
	location = {Munich, Germany}
}
@InProceedings{LQ-nets,
	author = {Zhang, Dongqing and Yang, Jiaolong and Ye, Dongqiangzi and Hua, Gang},
	title = {LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks},
	booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
	month = {September},
	year = {2018}
}

@article{DoReFa,
  author    = {Shuchang Zhou and
               Zekun Ni and
               Xinyu Zhou and
               He Wen and
               Yuxin Wu and
               Yuheng Zou},
  title     = {DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with
               Low Bitwidth Gradients},
  journal   = {CoRR},
  volume    = {abs/1606.06160},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.06160},
  eprinttype = {arXiv},
  eprint    = {1606.06160}
}
@inproceedings{ABC-net,
	 author = {Lin, Xiaofan and Zhao, Cong and Pan, Wei},
	 booktitle = {Advances in Neural Information Processing Systems},
	 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	 pages = {},
	 publisher = {Curran Associates, Inc.},
	 title = {Towards Accurate Binary Convolutional Neural Network},
	 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b1a59b315fc9a3002ce38bbe070ec3f5-Paper.pdf},
	 volume = {30},
	 year = {2017}
}
@article{INQ,
 	 title={Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights},
 	 author={Aojun Zhou and Anbang Yao and Yiwen Guo and Lin Xu and Yurong Chen},
 	 journal={ArXiv},
 	 year={2017},
 	 volume={abs/1702.03044}
}
@inproceedings{BNN,
 	author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
 	booktitle = {Advances in Neural Information Processing Systems},
	 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	 pages = {},
	 publisher = {Curran Associates, Inc.},
	 title = {Binarized Neural Networks},
	 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf},
	 volume = {29},
	 year = {2016}
}
@inproceedings{XNOR-net,
    Author = {Mohammad Rastegari and Vicente Ordonez and Joseph Redmon and Ali Farhadi},
    Title = {XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks},
    Booktitle = {ECCV},
    Year = {2016}
}
@inproceedings{TBN,
  title={TBN: Convolutional Neural Network with Ternary Inputs and Binary Weights},
  author={Diwen Wan and Fumin Shen and Li Liu and Fan Zhu and Jie Qin and Ling Shao and Heng Tao Shen},
  booktitle={European Conference on Computer Vision},
  year={2018}
}

@article{TWN,
  author    = {Fengfu Li and
               Bin Liu},
  title     = {Ternary Weight Networks},
  journal   = {CoRR},
  volume    = {abs/1605.04711},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.04711},
  eprinttype = {arXiv},
  eprint    = {1605.04711}
}

@article{FP-BNN,
	title = {FP-BNN: Binarized neural network on FPGA},
	journal = {Neurocomputing},
	volume = {275},
	pages = {1072-1086},
	year = {2018},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2017.09.046},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231217315655},
	author = {Shuang Liang and Shouyi Yin and Leibo Liu and Wayne Luk and Shaojun Wei}
}

@inproceedings{lowrank,
	 author = {Indyk, Piotr and Vakilian, Ali and Yuan, Yang},
 	booktitle = {Advances in Neural Information Processing Systems},
 	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 	pages = {},
 	publisher = {Curran Associates, Inc.},
 	title = {Learning-Based Low-Rank Approximations},
 	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/1625abb8e458a79765c62009235e9d5b-Paper.pdf},
 	volume = {32},
 	year = {2019}
}
@InProceedings{SVD,
	author={Bermeitinger, Bernhard and Hrycej, Tomas and Handschuh, Siegfried},
	title={Singular Value Decomposition and Neural Networks},
	booktitle={rtificial Neural Networks and Machine Learning -- ICANN 2019: Deep Learning},
	year={2019},
	publisher={Springer International Publishing},
	pages={153--164},
}

@inproceedings{Tucker,
	 author = {Malik, Osman Asif and Becker, Stephen},
	 booktitle = {Advances in Neural Information Processing Systems},
	 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	 pages = {},
	 publisher = {Curran Associates, Inc.},
	 title = {Low-Rank Tucker Decomposition of Large Tensors Using TensorSketch},
	 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/45a766fa266ea2ebeb6680fa139d2a3d-Paper.pdf},
	 volume = {31},
	 year = {2018}
}

@article{TensorTrain,
	author = {Oseledets, I. V.},
	title = {Tensor-Train Decomposition},
	journal = {SIAM Journal on Scientific Computing},
	volume = {33},
	number = {5},
	pages = {2295-2317},
	year = {2011},
	doi = {10.1137/090752286},
}
@INPROCEEDINGS{CP,
  author={Ma, Ruixin and Wang, Chuang and Li, Xin},
  booktitle={2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)}, 
  title={CP Decomposition for Fast training of Bi-LSTM}, 
  year={2021},
  volume={},
  number={},
  pages={244-248},
  doi={10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00051}}

@InProceedings{BT,
	author = {Ye, Jinmian and Wang, Linnan and Li, Guangxi and Chen, Di and Zhe, Shandian and Chu, Xinqi and Xu, Zenglin},
	title = {Learning Compact Recurrent Neural Networks With Block-Term Tensor Decomposition},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2018}
}

@InProceedings{QuantCVPR,
	author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
	title = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2018}
}

@inproceedings{HitNet,
 	author = {Wang, Peiqi and Xie, Xinfeng and Deng, Lei and Li, Guoqi and Wang, Dongsheng and Xie, Yuan},
 	booktitle = {Advances in Neural Information Processing Systems},
 	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 	pages = {},
	publisher = {Curran Associates, Inc.},
 	title = {HitNet: Hybrid Ternary Recurrent Neural Network},
 	url = {https://proceedings.neurips.cc/paper/2018/file/82cec96096d4281b7c95cd7e74623496-Paper.pdf},
 	volume = {31},
 	year = {2018}
}

@inproceedings{TernaryQuanti,
	title={Trained Ternary Quantization},
	author={Chenzhuo Zhu and Song Han and Huizi Mao and William J. Dally},
	booktitle={International Conference on Learning Representations},
	year={2017},
	url={https://openreview.net/forum?id=S1_pAu9xl}
}

@ARTICLE{E-LSTM,
  author={Wang, Meiqi and Wang, Zhisheng and Lu, Jinming and Lin, Jun and Wang, Zhongfeng},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems}, 
  title={E-LSTM: An Efficient Hardware Architecture for Long Short-Term Memory}, 
  year={2019},
  volume={9},
  number={2},
  pages={280-291},
  doi={10.1109/JETCAS.2019.2911739}
}

@article{Lacey,
  title={Deep Learning on FPGAs: Past, Present, and Future},
  author={Griffin Lacey and Graham W. Taylor and Shawki Areibi},
  journal={ArXiv},
  year={2016},
  volume={abs/1602.04283}
}

@article{DeepRnn,
  title={Hardware accelerators for recurrent neural networks on FPGA},
  author={Andre Xian Ming Chang and Eugenio Culurciello},
  journal={2017 IEEE International Symposium on Circuits and Systems (ISCAS)},
  year={2017},
  pages={1-4}
}


@ARTICLE{WangLong:TNNLS'23,
  author={Wang, Hai and Long, Xingyi and Liu, Xue-Xin},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={fastESN: Fast Echo State Network}, 
  year={2022},
  volume={},
  number={},
  pages={1-15},
  doi={10.1109/TNNLS.2022.3167466}}

@inproceedings{AlexNet,
 	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 	booktitle = {Advances in Neural Information Processing Systems},
 	editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 	pages = {},
 	publisher = {Curran Associates, Inc.},
 	title = {ImageNet Classification with Deep Convolutional Neural Networks},
 	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 	volume = {25},
 	year = {2012}
}
@INPROCEEDINGS{TensorCore,
  author={Markidis, Stefano and Chien, Steven Wei Der and Laure, Erwin and Peng, Ivy Bo and Vetter, Jeffrey S.},
  booktitle={2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
  title={NVIDIA Tensor Core Programmability, Performance \& Precision}, 
  year={2018},
  volume={},
  number={},
  pages={522-531},
  doi={10.1109/IPDPSW.2018.00091}}

@article{Cudnn,
  author    = {Sharan Chetlur and
               Cliff Woolley and
               Philippe Vandermersch and
               Jonathan Cohen and
               John Tran and
               Bryan Catanzaro and
               Evan Shelhamer},
  title     = {cuDNN: Efficient Primitives for Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1410.0759},
  year      = {2014},
  url       = {http://arxiv.org/abs/1410.0759},
  eprinttype = {arXiv},
  eprint    = {1410.0759}
}

@inproceedings{AEC,
	author = {Baldi, Pierre},
	title = {Autoencoders, Unsupervised Learning and Deep Architectures},
	year = {2011},
	publisher = {JMLR.org},
	abstract = {Autoencoders play a fundamental role in unsupervised learning and in deep architectures for transfer learning and other tasks. In spite of their fundamental role, only linear autoencoders over the real numbers have been solved analytically. Here we present a general mathematical framework for the study of both linear and non-linear autoencoders. The framework allows one to derive an analytical treatment for the most non-linear autoencoder, the Boolean autoencoder. Learning in the Boolean autoencoder is equivalent to a clustering problem that can be solved in polynomial time when the number of clusters is small and becomes NP complete when the number of clusters is large. The framework sheds light on the different kinds of autoencoders, their learning complexity, their horizontal and vertical composability in deep architectures, their critical points, and their fundamental connections to clustering, Hebbian learning, and information theory.},
	booktitle = {Proceedings of the 2011 International Conference on Unsupervised and Transfer Learning Workshop - Volume 27},
	pages = {37–50},
	numpages = {14},
	keywords = {compression, information theory, hebbian learning, boolean, unsupervised learning, principal component analysis, complexity, autoencoders, clustering, deep architectures},
	location = {Washington, USA},
	series = {UTLW'11}
}

@inproceedings{DeepLstm,
  author={Haşim, Sak and Andrew, Senior and Kanishka, Rao and Françoise, Beaufays},
  title={{Fast and accurate recurrent neural network acoustic models for speech recognition}},
  year={2015},
  booktitle={Proc. Interspeech 2015},
  pages={1468--1472},
  doi={10.21437/Interspeech.2015-350}
}
@article{DeepESN,
	title = {Design of deep echo state networks},
	journal = {Neural Networks},
	volume = {108},
	pages = {33-47},
	year = {2018},
	issn = {0893-6080},
	doi = {https://doi.org/10.1016/j.neunet.2018.08.002},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608018302223},
	author = {Claudio, Gallicchio and Alessio, Micheli and Luca,Pedrelli}
}
@article{DEIM,
  title={Nonlinear model reduction via discrete empirical interpolation},
  author={Chaturantabut, Saifon and Sorensen, Danny C},
  journal={SIAM Journal on Scientific Computing},
  volume={32},
  number={5},
  pages={2737--2764},
  year={2010},
  publisher={SIAM}
}

@book{zhu1973wulixue,
  title = {物理学},
  author = {竺可桢},
  year = {1973},
  address = {北京},
  pages = {56-60},
  publisher = {科学出版社}
}

@thesis{chen2001hao,
  author = {陈念永},
  title = {毫米波细胞生物效应及抗肿瘤研究},
  institution = {电子科技大学},
  year = {2001},
  pages = {50-60},
  address = {成都}
}

@newspaper{gu2012lao,
  author = {顾春},
  title = {牢牢把握稳中求进的总基调},
  journal = {人民日报},
  date = {2012年3月31日}
}

@techreport{feng997he,
  author = {冯西桥},
  title = {核反应堆压力容器的{LBB}分析},
  institution = {清华大学核能技术设计研究院},
  date = {1997年6月25日},
  address = {北京}
}

@patent{xiao2012yi,
  author = {肖珍新},
  title = {一种新型排渣阀调节降温装置},
  date = {2012年4月25日},
  type = {实用新型专利},
  country = {中国},
  id = {ZL201120085830.0}
}

@standard{zhong1994zhong,
  institution = {中华人民共和国国家技术监督局},
  id = {GB3100-3102},
  title = {中华人民共和国国家标准--量与单位},
  publisher = {中国标准出版社},
  date = {1994年11月1日},
  address = {北京}
}

@digital{clerc2010discrete,
  author = {M. Clerc},
  title = {Discrete particle swarm optimization: a fuzzy combinatorial box},
  type = {EB/OL},
  date = {July 16, 2010},
  url = {http://clere.maurice.free.fr/pso/Fuzzy_Discrere_PSO/Fuzzy_DPSO.htm}
}
