@article{Hornik,
  title = {Multilayer Feedforward Networks are Universal Approximators},
  author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
  journal = {IEEE Transactions on Neural Networks},
  volume = {2},
  number = {5},
  pages = {359 -- 366},
  year = {1989}
}

@book{ComputerArchi,
  title = {Computer Architecture: A Quantitative Approach},
  author = {John Hennessy and David Patterson},
  year = {2017},
  address = {America},
  pages = {1-2},
  publisher = {Elsevier}
}


@article{Bengio1994b,
  title = {Learning long-term dependencies with gradient descent is difficult},
  author = {Y. Bengio and P. Simard and P. Frasconi},
  journal = {IEEE Transactions on Neural Networks},
  volume = {5},
  number = {2},
  pages = {157 -- 166},
  year = {1994}
}

@article{Hochreiter,
	author = {Hochreiter, Sepp},
	journal = {docter degree},
	year = {1991},
	month = {04},
	pages = {},
	title = {Untersuchungen zu dynamischen neuronalen Netzen}
}



@article{NAS_ICLR,
	author = {Zoph, Barret and Le, Quoc},
	journal = {Proc. ICLR},
	year = {2017},
	month = {4},
	pages = {1-2},
	title = {Neural Architecture Search with Reinforcement Learning}
}

@conference{NAS_CVPR,
  author={Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Learning Transferable Architectures for Scalable Image Recognition}, 
  year={2018},
  volume={},
  number={},
  pages={8697-8710},
  doi={10.1109/CVPR.2018.00907}
}

@article{MobileNetsV1,
  author    = {Andrew G. Howard and
               Menglong Zhu and
               Bo Chen and
               Dmitry Kalenichenko and
               Weijun Wang and
               Tobias Weyand and
               Marco Andreetto and
               Hartwig Adam},
  title     = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
               Applications},
  journal   = {CoRR},
  volume    = {abs/1704.04861},
  year      = {2017},
}

@conference{MobileNetsV2,
	author = {M. Sandler and A. Howard and M. Zhu and A. Zhmoginov and L. Chen},
	booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	title = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
	year = {2018},
	volume = {},
	issn = {},
	pages = {4510-4520},
	doi = {10.1109/CVPR.2018.00474},
	publisher = {IEEE Computer Society},
	month = {6}
}

@conference{Shuffnet,
  author={Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices}, 
  year={2018},
  volume={},
  number={},
  pages={6848-6856},
  doi={10.1109/CVPR.2018.00716}
}

@conference{CompRNN,
	author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
	title = {An Empirical Exploration of Recurrent Network Architectures},
	year = {2015},
	publisher = {JMLR.org},
	booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
	pages = {2342–2350},
	location = {Lille, France},
}


@inproceedings{WeightPurning,
 author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning both Weights and Connections for Efficient Neural Network},
 url = {https://proceedings.neurips.cc/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf},
 volume = {28},
 year = {2015}
}

@ARTICLE{Purning,
  author={Karnin, E.D.},
  journal={IEEE Transactions on Neural Networks}, 
  title={A simple procedure for pruning back-propagation trained neural networks}, 
  year={1990},
  volume={1},
  number={2},
  pages={239-242},
  doi={10.1109/72.80236}
}

@inproceedings{ESE,
	author = {Han, Song and Kang, Junlong and Mao, Huizi and Hu, Yiming and Li, Xin and Li, Yubin and Xie, Dongliang and Luo, Hong and Yao, Song and Wang, Yu and Yang, Huazhong and Dally, William (Bill) J.},
	title = {ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA},
	year = {2017},
	isbn = {9781450343541},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3020078.3021745},
	doi = {10.1145/3020078.3021745},
	booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	pages = {75–84},
	location = {Monterey, California, USA},
	series = {FPGA '17}
}

@inproceedings{C-LSTM,
	author = {Wang, Shuo and Li, Zhe and Ding, Caiwen and Yuan, Bo and Qiu, Qinru and Wang, Yanzhi and Liang, Yun},
	title = {C-LSTM: Enabling Efficient LSTM Using Structured Compression Techniques on FPGAs},
	year = {2018},
	isbn = {9781450356145},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3174243.3174253},
	doi = {10.1145/3174243.3174253},
	booktitle = {Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	pages = {11–20},
	location = {Monterey, CALIFORNIA, USA},
	series = {FPGA '18}
}

@inproceedings{Cao,
	author = {Cao, Shijie and Zhang, Chen and Yao, Zhuliang and Xiao, Wencong and Nie, Lanshun and Zhan, Dechen and Liu, Yunxin and Wu, Ming and Zhang, Lintao},
	title = {Efficient and Effective Sparse LSTM on FPGA with Bank-Balanced Sparsity},
	year = {2019},
	isbn = {9781450361378},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3289602.3293898},
	doi = {10.1145/3289602.3293898},
	abstract = {Neural networks based on Long Short-Term Memory (LSTM) are widely deployed in latency-sensitive language and speech applications. To speed up LSTM inference, previous research proposes weight pruning techniques to reduce computational cost. Unfortunately, irregular computation and memory accesses in unrestricted sparse LSTM limit the realizable parallelism, especially when implemented on FPGA. To address this issue, some researchers propose block-based sparsity patterns to increase the regularity of sparse weight matrices, but these approaches suffer from deteriorated prediction accuracy. This work presents Bank-Balanced Sparsity (BBS), a novel sparsity pattern that can maintain model accuracy at a high sparsity level while still enable an efficient FPGA implementation. BBS partitions each weight matrix row into banks for parallel computing, while adopts fine-grained pruning inside each bank to maintain model accuracy. We develop a 3-step software-hardware co-optimization approach to apply BBS in real FPGA hardware. First, we propose a bank-balanced pruning method to induce the BBS pattern on weight matrices. Then we introduce a decoding-free sparse matrix format, Compressed Sparse Banks (CSB), that transparently exposes inter-bank parallelism in BBS to hardware. Finally, we design an FPGA accelerator that takes advantage of BBS to eliminate irregular computation and memory accesses. Implemented on Intel Arria-10 FPGA, the BBS accelerator can achieve 750.9 GOPs on sparse LSTM networks with a batch size of 1. Compared to state-of-the-art FPGA accelerators for LSTM with different compression techniques, the BBS accelerator achieves 2.3 ~ 3.7x improvement on energy efficiency and 7.0 ~ 34.4x reduction on latency with negligible loss of model accuracy.},
	booktitle = {Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	pages = {63–72},
	location = {Seaside, CA, USA},
	series = {FPGA '19}
}

@article{AutoMl1,
	title = {AutoPruner: An end-to-end trainable filter pruning method for efficient deep model inference},
	journal = {Pattern Recognition},
	volume = {107},
	pages = {107461},
	year = {2020},
	issn = {0031-3203},
	doi = {https://doi.org/10.1016/j.patcog.2020.107461},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320302648},
	author = {Jian-Hao Luo and Jianxin Wu}
}

@inproceedings{AutoMl2,
	author = {He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
	title = {AMC: AutoML for Model Compression and Acceleration on Mobile Devices},
	year = {2018},
	isbn = {978-3-030-01233-5},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	url = {https://doi.org/10.1007/978-3-030-01234-2_48},
	doi = {10.1007/978-3-030-01234-2_48},
	booktitle = {Computer Vision – ECCV 2018: 15th European Conference, Munich, Germany, September 8–14, 2018, Proceedings, Part VII},
	pages = {815–832},
	location = {Munich, Germany}
}

@article{FP-BNN,
	title = {FP-BNN: Binarized neural network on FPGA},
	journal = {Neurocomputing},
	volume = {275},
	pages = {1072-1086},
	year = {2018},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2017.09.046},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231217315655},
	author = {Shuang Liang and Shouyi Yin and Leibo Liu and Wayne Luk and Shaojun Wei}
}



@InProceedings{QuantCVPR,
	author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
	title = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2018}
}

@inproceedings{HitNet,
 	author = {Wang, Peiqi and Xie, Xinfeng and Deng, Lei and Li, Guoqi and Wang, Dongsheng and Xie, Yuan},
 	booktitle = {Advances in Neural Information Processing Systems},
 	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 	pages = {},
	publisher = {Curran Associates, Inc.},
 	title = {HitNet: Hybrid Ternary Recurrent Neural Network},
 	url = {https://proceedings.neurips.cc/paper/2018/file/82cec96096d4281b7c95cd7e74623496-Paper.pdf},
 	volume = {31},
 	year = {2018}
}

@inproceedings{TernaryQuanti,
	title={Trained Ternary Quantization},
	author={Chenzhuo Zhu and Song Han and Huizi Mao and William J. Dally},
	booktitle={International Conference on Learning Representations},
	year={2017},
	url={https://openreview.net/forum?id=S1_pAu9xl}
}

@ARTICLE{E-LSTM,
  author={Wang, Meiqi and Wang, Zhisheng and Lu, Jinming and Lin, Jun and Wang, Zhongfeng},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems}, 
  title={E-LSTM: An Efficient Hardware Architecture for Long Short-Term Memory}, 
  year={2019},
  volume={9},
  number={2},
  pages={280-291},
  doi={10.1109/JETCAS.2019.2911739}
}

@article{Lacey,
  title={Deep Learning on FPGAs: Past, Present, and Future},
  author={Griffin Lacey and Graham W. Taylor and Shawki Areibi},
  journal={ArXiv},
  year={2016},
  volume={abs/1602.04283}
}

@article{DeepRnn,
  title={Hardware accelerators for recurrent neural networks on FPGA},
  author={Andre Xian Ming Chang and Eugenio Culurciello},
  journal={2017 IEEE International Symposium on Circuits and Systems (ISCAS)},
  year={2017},
  pages={1-4}
}

@ARTICLE{WangLong:TNNLS'23,
	author = {Hai Wang and Xingyi Long and Xue-Xin Liu},
	journal = tnnls,
	title = {{fastESN}: Fast Echo State Network},
	year = {2022},
        OPTmonth = {October},
        OPTvolume = {70},
        OPTnumber = {10},
        OPTpages = {1539--1554},
}

@article{wang1999sanwei,
  title = {三维矢量散射积分方程中奇异性分析},
  author = {王浩刚 and 聂在平},
  journal = {电子学报},
  volume = {27},
  number = {12},
  pages = {68 -- 71},
  year = {1999}
}

@conference{liuxf2006,
  author = {Liu, X F and Wang, Bing Zhong and Shao, Wei and Wen Wang},
  title = {A marching-on-in-order scheme for exact attenuation constant extraction of lossy transmission lines},
  year = {2006},
  pages = {527-529},
  address = {Chengdu},
  booktitle = {China-Japan Joint Microwave Conference Proceedings}
}

@book{zhu1973wulixue,
  title = {物理学},
  author = {竺可桢},
  year = {1973},
  address = {北京},
  pages = {56-60},
  publisher = {科学出版社}
}

@thesis{chen2001hao,
  author = {陈念永},
  title = {毫米波细胞生物效应及抗肿瘤研究},
  institution = {电子科技大学},
  year = {2001},
  pages = {50-60},
  address = {成都}
}

@newspaper{gu2012lao,
  author = {顾春},
  title = {牢牢把握稳中求进的总基调},
  journal = {人民日报},
  date = {2012年3月31日}
}

@techreport{feng997he,
  author = {冯西桥},
  title = {核反应堆压力容器的{LBB}分析},
  institution = {清华大学核能技术设计研究院},
  date = {1997年6月25日},
  address = {北京}
}

@patent{xiao2012yi,
  author = {肖珍新},
  title = {一种新型排渣阀调节降温装置},
  date = {2012年4月25日},
  type = {实用新型专利},
  country = {中国},
  id = {ZL201120085830.0}
}

@standard{zhong1994zhong,
  institution = {中华人民共和国国家技术监督局},
  id = {GB3100-3102},
  title = {中华人民共和国国家标准--量与单位},
  publisher = {中国标准出版社},
  date = {1994年11月1日},
  address = {北京}
}

@digital{clerc2010discrete,
  author = {M. Clerc},
  title = {Discrete particle swarm optimization: a fuzzy combinatorial box},
  type = {EB/OL},
  date = {July 16, 2010},
  url = {http://clere.maurice.free.fr/pso/Fuzzy_Discrere_PSO/Fuzzy_DPSO.htm}
}
