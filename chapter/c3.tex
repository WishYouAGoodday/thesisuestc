\chapter{算法分析及系统架构设计}
本章将针对循环神经网络前向传播过程提出一套从软件算法，硬件实现到系统集成的全流程设计框架和独立整的解决方案。不同于所有过往的压缩加速设计，
本文所采用的压缩算法具有压缩成本小，无需训练集等优势，可以不依赖云端算力支持，仅在分布式的边缘设备实现，算法的详细介绍将在3.1节展开。算法特性
决定了其适用场景，而具有弹性性能需求的应用场景又是普遍存在的，所以在现实条件的约束下，本文对算法进行了合理的拆分与协同并分别映射到软件和硬件上
进行执行，具体的系统架构设计见3.2节。最后，面向算法和应用场景需求，本文进行了定制化专用硬件架构的设计，通过对上层应用的需求和硬件加速空间
进行分析，本文创造性的提出了一种具有紧凑计算结构和可配置功能模块的循环神经网络前向传播硬件加速器。该加速器能通过简单的配置和数据重载完成不同网络结构
和不同模型大小的切换，相比传统的神经网络加速器的一种硬件实现对应唯一的网络结构和模型尺寸，本文所设计的加速器即具有“专用”所带来的高效性，
又具有一定程度“通用”所带来的灵活性，能满足弹性应用场景下性能可调的需求。在众多的循环神经网络模型中，本文选取回声状态网络作为设计实例，因其结构简单，
训练成本低和应用前景开阔等优势，同时又极具代表性和紧迫性，详细分析说明可见2.1节。对于其他种类的循环神经网络，除具体实现细节存在微小差异外，本文在
系统层面提出的设计方案，软件算法和硬件架构设计也同样适用，因此，本文不再一一说明。
\section{基于投影的模型压缩算法}
本小节先展示模型压缩的效果---简化网络结构，然后就简化网络的生成过程(状态投影和激活函数近似)作详细的说明，最后会评估与分析压缩算法的特性和作用效果。
以上描述的模型压缩的全景图是本文后续工作的基础。
\subsection{简化网络结构}
回声状态网络在模型压缩算法的作用下生成了简化网络，其数学模型为式\ref{eq:redesn}，式中\(\widehat{x},\widehat{z} \in \mathbb{R}^q\)表示简化网络隐藏层的状态，
\(\widehat{W}, \widehat{E}_d \in \mathbb{R}^{q \times q}\)表示隐藏层与隐藏层的连接权重，\(\widehat{E}_l,\in \mathbb{R}^{q \times q}\)表示隐藏层跨时间步的自循环矩阵，
\(\widehat{W}_{in},\widehat{W}_{out}\in \mathbb{R}^{q \times n_{in}}\)分别表示输入到隐藏层以及隐藏层到输出的连接权重，\(q\)是隐藏层的神经元数量，
\(n_{in}\)为输入数量，\(n_{out}\)为输出数量。
\begin{equation}\label{eq:redesn}
	\begin{split}
		\widehat{z}_t &= f(\widehat{W} * \widehat{x}_{t-1} + \widehat{W}_{in} * u_{t})				\\	
		\widehat{x}_t &= \widehat{E}_l * \widehat{x}_{t-1} + \widehat{E}_d * \widehat{z}_{t} 		\\
		y_{t} 			&= \widehat{W}_{out} * \widehat{x}_{t}	
	\end{split}
\end{equation}

相较于回声状态网络的原网络模型，简化网络的数学描述更加复杂， 具体表现为引入了新的状态变量\(\widehat{z}\)和新的新的拓扑连接\(\widehat{E}_l,\widehat{E}_d\)。
\(\widehat{z}\)是输入\(u\)和状态\(\widehat{x}\)的函数，而\(\widehat{x}\)又与暂态\(\widehat{z}\)呈线性相关的关系。这些新引入的变量和关系
改变了回声状态网络的模型结构，即简化网络在原网络结构的基础上增加了一层隐藏层，如图\ref{fig:esn_convert}所示。此外，简化网络拥有两个循环结构：暂态\(\widehat{z}\)
和状态\(\widehat{x}\)之间的层间循环体和状态层\(\widehat{x}\)内部的自循环体。循环体的增加赋予了循环神经网络更强大的记忆能力，这使得少量的隐藏层神经元
就可以实现对系统动态特性的表征。显然，本文所采用的模型压缩算法在网络结构上做出了让步，但是“祸兮，福之所倚”，简化网络在压缩神经元数量方面取得了巨大的成功。
二者利害相权的最终结果是：在模型预测精度不显著下降的情况下，简化网络的参数量将大幅减少，前向传播过程的时间复杂度也显著降低。
\begin{figure}
	\centering
	\includegraphics[width=0.7\columnwidth]{ESN_convert.eps}
	\caption{原始网络和简化网络的结构图}
	\label{fig:esn_convert}
\end{figure}

简化回声状态网络的结构如图\ref{fig:esn_red}所示。该网络的深度为四，包括输入层\(u\)，输出层\(y\)和两层隐藏层\(\widehat{z},\widehat{x}\)。
其中输入层，输出层和状态层\(\widehat{x}\)保留了原网络相似的结构和映射关系。暂态\(\widehat{z}\)是新引入的结构，其作用在于桥接输入层和状态层，
对流经该层的信息做初步的加工处理。暂态层\(\widehat{z}\)和状态层\(\widehat{x}\)共同组成隐藏层，数据在两层隐藏层之间双向流动，形成一个大的信息回路。
但是两层隐藏层也存在差异，暂态层不存在自循环体，因此传统上意义上不能称为状态，考虑到其也位于数据循环的关键路径，因此称之为暂态。实际上
仅从结构特性而言，暂态层和全连接层有更大的相似度。以上分析了简化网络的结构特性，相较于原网络，既存在保留部分，也添加了新的元素。 

\begin{figure}
	\centering
	\includegraphics[width=0.7\columnwidth]{ESN_red.eps}
	\caption{简化回声状态网络结构}
	\label{fig:esn_red}
\end{figure}

简化网络的前向传播速度相较原网络会大幅提升，这得益于隐藏层神经元数量的减少。简化网络内部神经元的数量每层\(q\)个，远小于原网络的\(n\)个。
相应的，网络中神经元的连接复杂度也将降低，权重矩阵的参数量将从\(n^2\)变为\(q^2\)。以上神经元数量和权重参数量的压缩将会直观的反映到模型的
计算复杂度上，原网络每个时间步长的前向传播计算复杂度为
\begin{equation}
	\mathcal{O}(n^2 + n*n_{in} + n*n_{out})
\end{equation}
简化网络的前向传播计算复杂度为
\begin{equation}
	\mathcal{O}(q^2 + q*n_{in} + q*n_{out})
\end{equation}
在输入输出数量不变的条件下，简化网络的时间复杂度仅由\(q\)决定，和原网络的阶数\(n\)无关。又因为\(q<<n\)，所以简化网络前向传播的速度会比原网络快很多。
具体的\(q\)值的选取需根据用户对精度和速度的需求来确定，简化网络的阶数\(q\)越小，模型的计算速度越快；\(q\)越大，模型的精度越高。

\subsection{网络压缩}
前面的叙述介绍了简化回声状态网络和原网络之间在网络结构上的相似性和差异性，解释了简化网络模型预测精度不会显著下降的原因，并从计算复杂度的角度说明了
简化网络在前向传播过程中加速能力的来源，下面本小节将就原网络如何生成简化网络进行简要的说明，模型压缩算法的具体推导请见文献\citing{WangLong:TNNLS'23}。
%介绍高速回声状态网络的生成，算法顺序介绍，or硬件需求倒推
\subsubsection{状态近似}
为了记忆动态系统丰富的时序特性，循环神经网络一般拥有庞大数量的隐藏层神经元，这些神经元互连关系复杂，对信息拥有记忆和遗忘功能，是循环神经
网络最重要的组成部分。对于简单的任务，仅需要少量的神经元便可完成，而复杂的任务则需要更多神经元协同。实际情况下，任务的复杂程度判别具有
相当程度的主观性，隐藏层神经元数量会被保守的设置为远超实际需求的数量，即存在大量的冗余性。本小节针对神经元的冗余性，采用基于投影的状态近似方法进行消除。
状态近似框架：
\begin{equation}
	V_x * \widehat{x}_{t} \approx x_{t}
\end{equation}
其中\(V_x \in \mathbb{R}^{n \times q}\)是状态投影矩阵,满足\(V_x^T * V_x = I\)，\(x_{t} \in \mathbb{R}^n\)是状态向量，\(\widehat{x}_t \in \mathbb{R}^q\)是状态近似向量。
状态近似将高维向量\(x_t\)投影到状态空间中，并用状态空间的基向量进行线性表示，其坐标为\(\widehat{x}\)。实际上，由于输出只需要选择性的保留
动态系统过去序列的某些方面信息，而与其他方面无关，系统的动态特征往往有限且集中。这在状态空间上表现为：和任务输入输出相关的状态只占全部状态的一部分，即
有效状态空间是满状态空间的子空间。按照状态子空间中特征的重要程度划分，有效状态空间又可以分为核心特性空间和外围特征空间。在进行状态投影时，状态向量将首先
被投影到核心特征空间中，在这种状态近似情况下，模型的预测不会偏离真实输出太大；其次，状态将会被投影到外围特征空间，该空间越大，模型就越能捕捉系统微小
的动态特征，模型的输出也就越具有丰富性。状态子空间的选取将会对状态近似后的网络预测效果产生较大的影响，如何构造状态空间以实现较好的近似将在3.1.2.3中介绍。\\
将状态近似代入回声状态网络模型\ref{eq:esn}得到
\begin{equation}
	\begin{split}
		\widehat{x}_{t} & = V_x^T * f(W*V_x * \widehat{x}_{t-1} + W_{in} * u_t)	\\
					y_t & = W_{out} *V_x * \widehat{x}_t
	\end{split}
\end{equation}
基于投影的状态近似方法生成的简化网络除了隐藏层神经元数量更少以外，还在自循环结构上增加了线性算子。该算子需要在状态激活后
作用于每一个隐藏层神经元，并且运算次数为\(n\)，这导致隐藏层的等效神经元数量也为\(n\)。为了获取真正的模型简化，一个直观的想法是将\(V_x^T\)
移动到激活函数内部，形成类似\(V_x^T * W * V_x\)的\(q \times q\)矩阵，这样模型压缩后等效神经元的数量也为\(q\)，简化网络前向传播的时间复杂度
也将进一步降低。然而循环神经网络的激活函数通常是非线性的，非线性算子和线性算子的执行顺序是不能更改的，因此此想法无法直接实现。

如果存在矩阵\(P \in \mathbb{R}^{n \times q}\) 能够和非线性运算交换顺序，即矩阵能够穿透激活函数，使得
\begin{equation}
	P^T*f(W*V_x *\widehat{x}_t + W_{in} *u_t) = f(P^T*W*V_x*\widehat{x}_t + P^T *W_{in}* u_t)	
\end{equation}
那么将会实现回声状态网络真正的简化，其模型大小就完全不受原网络尺寸的影响。如何构造这样一个理想的矩阵，数学上早已给出了充分的研究和证明。
当\(P = [e_1,e_2,...,e_q]\) (\(e \in \mathbb{R}^n\)是只含一个1且其余元素为0的向量)，线性运算和非线性运算可以交换顺序。其原理在于矩阵P的
运算只是从n行向量中选出q行，不会改变向量中具体元素的数值，因此不会影响激活函数\(f\)独立的应用到向量的每一个元素。在回声状态网络的数学方程中，
矩阵P的功能是从n个方程中选择q个方程，使得这q个方程的近似解和n个方程的真实解误差最小。关于矩阵P的构造，可以通过离散经验插值方法（discrete empirical interpolation method，DEIM）
实现，DEIM的完整讨论请参见\citing{}

\subsubsection{激活函数近似}
投影方法同样也适用于激活函数的近似，\(n\)维的函数\(g(x)\)会被用\(q\)维函数\(\widehat{g}(x)\)替换，这可以减小激活函数的应用次数。但是简单直接的对非线性激活函数
使用投影近似会带来系统不稳定的问题，系统稳定性分析请见文献[21]。因此为了得到渐进稳定的简化网络，激活函数将被分为线性部分和非线性部分，
投影近似仅作用于非线性部分。网络隐藏层非线性部分可以表示为
\begin{equation}\label{eq:h(x)}
	h(x_t) = f(W*x_t + W_{in}*u_t) - W*x_t
\end{equation}
结合激活函数近似框架
\begin{equation}
	V_h*\widehat{h}(x) \approx h(x)
\end{equation}
式中\(V_h \in \mathbb{R}^{n \times q}\)是函数投影矩阵，将选择矩阵P作用于该投影近似框架，得到
\begin{equation}\label{eq:Pselect}
	P_h^T*V_h*\widehat{h}(x) = P_h^T*h(x)
\end{equation}
最后，综合以上理论公式，将得到简化网络的最终形式
\begin{equation}\label{eq:esn_red}
	\begin{split}
		\widehat{x}_t  = &(V_x^T W V_X - \widehat{E}_d P_h^T W V_x)\widehat{x}_{t-1}  \\
					     & + \widehat{E}_{d} f(P_h^T W V_x \widehat{x}_{t-1} + P_h^{T} W_{in} u_{t})	\\
				y_t    = &W_{out} V_x \widehat{x}_t
	\end{split}
\end{equation}
其中，\(\widehat{E}_d = V_x^T V_h (P_h^TV_h)^{-1}\)，为进一步简化表达，定义
\begin{equation}
	\begin{split}
		\widehat{E}_l = V_x^T W V_x - \widehat{E}_d P_h^T W V_x,	\qquad &\widehat{W} = P_h^T W V_x,		\\
		\widehat{W}_{in} = P_h^T W_{in},							\qquad &\widehat{W}_{out} = W_{out}V_x
	\end{split}
\end{equation}
将以上表达代入(\ref{eq:esn_red})即可得到简化网络的模型(\ref{eq:redesn})

\subsubsection{状态采样和函数采样}
以上从数学的角度展示了回声状态网络如何一步一步的从原网络生成简化网络的过程，其中所使用的近似框架和DEIM是模型压缩中常见的方法，也适用于其他神经网络模型的压缩，
目前已经在LSTM，GRU等循环神经网络模型上检验过压缩效果。为使模型压缩的理论具有连续性，上述推导过程都是假设存在投影矩阵\(V_x,V_h\)以及选择矩阵\(P\)的基础上进行，
省略了如何构造这些矩阵的过程。然而，从工程的角度，如何构造这些矩阵并由这些矩阵生成简化网络的权重是更受关注的。本小节将通过状态采样和函数采样
的方法解决这一遗留问题。

状态采样是指在回声状态网络前向传播的过程中，采集一段不同时间点下的原始系统状态样本\(\{x_1,x_2,...,x_{n_s}\}\)形成状态空间。由于回声状态网络的隐藏层权重不可训练，
采样的状态空间也就和训练过程无关，其只决定于系统的初始化特性和输入序列特征。理论上，样本越丰富，状态空间就越能还原动力系统的真实特性。

同理，函数采样是指采样原网络前向传播过程中不同时刻的激活函数样本，实际上是激活函数近似框架中的非线性部分，如式\ref{eq:h(x)}所示。函数采样
将得到激活函数空间\(\{h_1,h_2,...,h_{n_s}\}\)。

在完成状态采样和激活函数采样后，将会对采样空间进行奇异值分解（SVD）以寻找可以作为投影面的一组标准正交基，如下所示：
\begin{equation}
	\begin{split}
		V_x \Sigma_x U_x^T \ \xleftarrow{SVD} \ X;	\qquad	H \ \xrightarrow{SVD} \ V_h \Sigma_h U_h^T 	
	\end{split}
\end{equation}
其中\(X = [x_1,x_2,...,x_{ns}] \in \mathbb{R}^{n \times n_s} \)，\(V_x \in \mathbb{R}^{n \times n_s},U_x \in \mathbb{R}^{n_s \times n_s}\)，\(\Sigma_x \in \mathbb{R}^{n \times n_s}\)
是状态空间及其分解的子空间；\(H = [h_1,h_2,...,h_{ns}] \in \mathbb{R}^{n \times n_s} \)，\(V_h \in \mathbb{R}^{n \times n_s},U_h \in \mathbb{R}^{n_s \times n_s}\)，\(\Sigma_h \in \mathbb{R}^{n \times n_s}\)
是激活函数空间和其分解的子空间。分解后的矩阵中，奇异值矩阵\(\Sigma\)和原矩阵是一一对应关系，左奇异矩阵和右奇异矩阵互为依赖，其作为一个整体和原矩阵保持对应关系。
由于这种对应关系的存在，以及其标准正交特性，奇异矩阵可以作为原空间的近似并且可以当作投影矩阵。本文将左奇异矩阵\(V\)作为投影矩阵。

为了减少与任务关联度低的特征在特征空间中所占的比例，这里选择保留\(V\)的前\(q\)列作为投影矩阵。
\begin{equation}
	V_x \ \leftarrow \ V_x[:,1:q];	\qquad V_h \ \leftarrow \ V_h[:,1:q]
\end{equation}
由于\(V\)的前q列对应\(\Sigma\)中最大的q个奇异值，所以此截断方法保留了原始矩阵最重要的信息，在特征空间中反映为：选取最重要的q个特征向量组成新的子空间，
尽管该特征子空间的维度远小于原始特征空间，但是高维特征却能以较小的精度损失投影到该子空间。关于q值的选取，往往不存在一个最优选项，需要根据
应用场景对精度的实际需求进行确定，q值越大，损失的信息越小，精度越高。

以上基于状态采样和函数采样获得了投影矩阵\(V_x,V_h\)，为了获取简化网络权重矩阵所需要的全部要素，还需要确定选择矩阵\(P_h\)。P首次出现在(\ref{eq:Pselect})，
其功能是从n个非线性方程中选出q个方程进行求解。由于每个方程有q个变量，并且非线性方程对应的函数是单调函数，因此求解出这q个变量实际所需的方程数也是q，但是
激活函数具有饱和特性，存在较大数值差异的自变量对应“相同”函数值的情况，因此需要找出这些方程，并选出其中一个方程作为代表，使得方程组的解和原方程组的
真实解误差最小。DEIM是一种基于贪婪思想从n个非线性方程中选择q个有精确“近似”方程的方法，该方法一个接一个的选出使得解向量的误最小的方程组，并最终选出q个符合要求的方程。
这里不再详细展示DEIM的推导过程，只展示本文用到的算法，如\ref{}所示。
\subsection{压缩算法的评估与析}
前面的小节展示了回声状态网络的简化网络结构，以及如何从原网络一步一步生成这样一个简化网络。本小节将对简化网络的实际运行效果进行评估，以证明
压缩算法的有效性和可行性。

首先，模型的压缩需要在保证模型的基本功能正常的前提下进行，脱离任务实际需求，仅考虑压缩率的模型压缩算法是往往无效的。图\ref{fig:accuracy}所示为简化网络
在NARMA10系统上的精度表现，其中原网络的模型大小为500阶，其模型预测值能较好的贴近系统真实值，尤其是在系统真实输出剧烈变化的位置，原网络能捕捉这种动态信息，
但是相对于其他变化平缓的区段，误差显得更大。简化网络的模型大小位80阶，远小于原网络的尺寸，但是其模型预测精度并没有大幅下降，图中显示简化网络
的预测值和系统真实输出也能保持较好的贴合度。在系统输出变化时，简化网络的输出会保持变化方向的一致性，尽管在数值上存在差异。由于简化网络是由原网络生成的，没有
在数据集上训练，因此原网络的精度是决定简化网络预测效果的直接因素，图中显示，简化网络的预测值和原网络的预测值在多数情况下能保持一致，除了个别位置波动较大。
误差是不可避免的，本文所使用的模型压缩算法能在可接受的误差范围内实现模型尺寸的压缩，是一种可行且有效的循环神经网络压缩方法。
\begin{figure}
	\centering
	\includegraphics[width=1.0\columnwidth]{exp/500&80_all.eps}
	\caption{ESN简化网络和原网络在NARMA10系统上的精度效果，原网络（OrgESN）为500阶，简化网络（RedESN）为80阶}
	\label{fig:accuracy}
\end{figure}

其次，模型压缩算法的目的是降低模型的时空复杂度，实现神经网络前向传播过程的加速。表\ref{tab:accuracy}所示为模型参数量，预测精度和速度。其中500表示原网络的尺寸，
其余尺寸代表不同阶数的简化网络。从参数量的角度，简化网络的参数量少于原网络，且压缩力度越大，参数量减少越明显。参数量的多少直接决定着模型
的存储开销，显然简化网络能有效节约存储空间。从推理速度的角度，简化网络的完成全部序列的推理所需要的时间明显小于原始网络，这是因为简化网络的的计算量更小。
最后从模型预测误差的角度，简化网络的精度损失更大，但是都没有超出可接受范围，并且随着模型尺寸的增加，精度也在上升。

综上所述，压缩算法在保证模型精度不显著下降的情况下，大幅的降低模型参数量，减轻了硬件计算平台存储和计算的压力，并最终达成了对模型前向传播过程加速的目的。
\input{exp/table_accuracy.tex}
\section{系统整体架构设计}
在前叙压缩算法的工作基础上，本章将对算法特性进行深入分析，并结合应用场景，提出一套针对循环神经网络在前向传播过程中的加速方案，该方案能根据用户的实际
性能需求匹配相应的加速能力，做到既不会性能过剩，也不会性能不足。然后，针对方案的具体实施，本章将从软硬件的角度考量方案各个组成部分实现的成本和收益，
并进行软硬件划分与协同，以最大化系统的实现效率。最后，本章将选用FPGA作为硬件计算平台，提出并设计循环神经网络加速系统的整体架构。
\subsection{前向传播及压缩流程}
本文采用基于投影的压缩算法进行模型压缩，压缩后简化网络的模型尺寸不受原网络的影响，并且该尺寸可以进行调节，当简化网络的尺寸设定为较大的
值时，可以实现较高的预测精度；当简化网络的尺寸设定为较小值时，模型前向传播的速度会加快。这种网络尺寸可调的特性赋予了神经网络在算法层面的弹性计算能力。
同时，随着模型尺寸的伸缩变化，网络的预测效果将会在某些方面得到增强，从而满足用户在特殊应用场景下的需求。通常情况，模型压缩算法都可以控制
压缩的尺度，并且产生不同的模型预测效果，例如剪枝技术可以调节模型的稀疏度，量化方法可以调节数据字长等。相较而言，本文的模型尺寸调节是指改变网络的结构，
与上述方法相互独立，并且由于其完善数学推导，在相同精度损失的条件下，往往能获得更高的模型压缩率。以上是简化网络前向传播过程的分析，其具有
模型尺寸可调的特点，能满足应用场景的弹性性能需求。

状态采样和激活函数采样形成的样本集是生成简化网络所需要的全部数据来源，采样过程是伴随原网络的前向传播过程同时发生，而与反向传播无关，因此压缩算法所
需的数据和模型训练集无关，更具体的，其仅决定于原网络的隐藏层特性和任务的输入序列特性。这种无数据集的特性赋予了压缩算法使用场景极大的自由度，
一些特殊场景如数据集获取成本大的场景，训练方法保密场景等往往不公布数据集，仅提供可前向传播的模型，传统的模型压缩算法将难以为继，而本文的压缩算法
依然可以发挥功效。以上是本文所采用算法的第二大特性---无需数据集，这使得算法的应用场景被拓宽。

投影方法包含矩阵分解和DEIM两大过程，相比于其他神经网络压缩算法需要反复迭代和微调等复杂流程，本文压缩算法的实现成本小。实际上，有关线性代数
在硬件上的加速与优化已经存在大量的成果，如BLAS\citing{}，LAPACK\citing{}，NumPy\citing{}，以及本文所使用的针对嵌入式设备而开发的函数库Eigen\citing{}。
丰富的开发工具和优化过的实现方案使得本文的压缩算法的实现成本进一步降低，甚至可以在资源有限的边缘的边缘设备上实现。以上是本文所采用算法的第三大特性
---压缩算法实现成本低，这使得网络的压缩过程可以在资源有限的场景使用。
\begin{figure}
	\centering
	\includegraphics[width=0.6\columnwidth]{exp/Flowchart_diagram.eps}
	\caption{循环神经网络前向传播加速和压缩流程图}
	\label{fig:flowchart}
\end{figure}

基于以上循环神经网络前向传播过程和压缩算法特性的深入分析，接下来，本文将提出一套符合算法特性，并结合实际应用场景的算法系统性实现方案。
该方案能根据应用场景实际需求动态地调节模型的尺寸，使得模型预测精度或速度恰好达到用户的要求。同时方案考虑了硬件计算平台的独立性，通过闭环的
系统流程实现了任务在边缘设备的完整运行。循环神经网络前向传播与加速的系统流程如图\ref{fig:flowchart}所示。首先需要向加速系统预存初始化参数
包括循环神经网络的权重参数(\(W,W_{in},W_{out}\))和模型压缩过程所需的投影矩阵(\(V_x,V_h\)) 及选择矩阵(\(P_h\))，然后系统会根据用户设定的模型尺寸生成
相应尺寸的简化网络权重参数，该过程只需要进行简单的矩阵乘法就可实现。最后，系统会执行简化网络的前向传播算法。在误差检测模块的作用下，当简化网络的误差较小时，
用户可以更改并减小模型尺寸以实现在保证模型精度的前提下最大化模型推理速度；当模型的误差较大且无法通过提高模型尺寸进行消除时，说明简化网络在模型压缩过程中丢失了这一段序列的
的动态信息，因此需要结合该段序列重新生成简化网络的权重参数。此时，系统将运行原始网络模型并对该段序列输入所产生的状态进行采样，形成状态样本集和
激活函数样本集，最后在状态近似和激活近似的框架下，样本集将生成\(V_x,V_h,P_h\)等压缩过程需要的全部信息。以上运行原网络并采样实现了系统和环境的数据交互，
并最终补偿了简化网络的精度，修正了初始化参数。
\subsection{软硬件功能划分}
在确定算法特性，应用场景需求和任务执行流程后，本小节将进一步确定系统的具体实现方式。通常情况，电子系统的实现包括软件和硬件两种方式。
软件实现是指采用C/C++等高级程序语言描述系统的行为，代码最终以串行执行的方式运行在通用处理器上，具有实现成本低，灵活性强等优势，擅长处理分支判断等
流程控制类任务，其劣势在于并行化程度低，计算周期长。硬件实现克服了软件的劣势，通过对资源进行空间复用，硬件实现可以轻易获得很高的并行度，从而实现
大规模批量化的处理数据。同时，相较软件执行过程，硬件实现通常是为具体应用量身定制，无需繁杂的编码解码过程，从而节省了能量消耗，具有高性能计算的特点。
然而，硬件实现方式也存在明显的劣势，包括灵活性差，资源开销大以及开发周期长等。因此，综合软硬件实现的优势，本文采用软硬件协同的方式对系统进行
设计实现。

系统功能的软硬件划分是否合理直接决定这系统能否高效运行。本文系统的功能划分如表\ref{tab:soft&hard}所示。首先，根据任务的目的对系统进行初步的划分，
系统被分解为推理部分和压缩部分两个主要部分。其中推理任务需要执行包括原始网络和简化网络两种不同结构模型的前向传播过程，而对简化网络前向传播过程进行加速是
本文系统设计的首要目标，并且其面临着低延迟，低功耗的约束，因此简化网络的前向传播将优先采用硬件实现的方式。从算法特性的角度，模型包含大量可并行计算
的单元，主要体现在矩阵向量乘法中，这能充分发挥硬件的优势。在简化网络实现方式确定的基础上，由于原始网络结构的基本组成单元和简化网络相同，因此针对结构上共性，
可以考虑共用一套硬件模块，对于少量存在较大差异的部分则单独设计硬件单元，但是额外的硬件开销应当尽量的减少，并且由于运行原始网络的目的不是用于实际的模型预测，而仅
用于采样压缩算法所需要的数据，因此节约硬件资源是首要设计目标。以上是对推理过程实现方式的分析，考虑到系统设计目标及实现的成本，系统将采用专用
硬件架构进行实现。模型压缩任务可以更具体的划分为矩阵奇异值分解（SVD），离散经验插值（DEIM）和简化网络生成。尽管这些功能模块都是将矩阵
作为基本操作对象，但是算法的复杂度高，并且从执行频次的角度，模型压缩的需求是存在的但却不会频繁地请求。因此，本文压缩过程的所有任务都将采用
软件的方式实现。

以上分析了软硬件实现方式的优势和劣势，并对系统功能进行了划分，综合考虑实现成本，硬件资源开销，系统目标及任务需求等方面因素，将系统各功能
模块映射到合适的实现方式上，初步完成了循环神经网络前向传播与加速的系统设计，本文将在后续章节详细的介绍具体实现细节。

\input{exp/table_hard&soft.tex}


\subsection{系统整体架构}
\begin{figure}
	\centering
	\includegraphics[width=0.8\columnwidth]{exp/fig_systemArch2.eps}
	\caption{系统整体架构图}
	\label{fig:sysArch}
\end{figure}


\section{面向算法的定制化硬件分析}
\subsection{算法需求分析}
1需要实现原始网络，高速网络两个网络结构。
2压缩网络结构尺寸可调节。
3模型压缩过程。
\subsection{硬件模块划分与复用性分析}
\subsection{激活函数分段近似}
\subsection{计算资源与存储资源需求分析}

\section{基于FPGA的加速器设计}
\subsection{硬件加速器整体架构}

\subsection{存储架构设计}

\subsection{计算架构设计}

\subsection{矩阵向量乘法模块}

\subsection{激活函数模块}

\subsection{IP核互联设计}

\section{本章小结}
