\chapter{绪\hspace{6pt}论}

\section{研究背景与意义}
人工智能（Artificial Intelligence，AI）自上世纪五十年代诞生以来，经过符号主义，联结主义，行为主义三大流派的发展，理论和技术取得了长足的进步。
近年来，基础算力的提升和大数据的兴起促使了人工智能新一轮的应用浪潮。
智能算法在语音语言，图像视频，推荐搜索，自动驾驶等领域都取得了革命性突破。
其中以深度神经网络为代表的算法相较传统算法表现出更优越的性能，在面对与日俱增的精度需求和纷繁复杂的任务场景时都能保持较好的学习和预测能力。

神经网络算法包含众多模型，其中广泛应用的网络包括前馈神经网络（feedforward neural network, FNN），循环神经网络（recurrent neural network, RNN），
卷积神经网络（convolutonal neural network, CNN），生成对抗深经网络（generative adversarial network，GAN）等。这些网络模型及其组合变体可针对性
的解决不同应用场景的任务需求。理论而言，只要数据集足够完备，模型规模足够大，神经网络可以作为万能近似器以任意精度逼近复杂函数\citing{Hornik}。
实际情况也正如此，为达到更高的精度和获得更好的模型泛化能力，神经网络朝着深层，大参数，高计算量和复杂结构的方向发展。
自2017年提出了基于注意力机制的Transformer网络结构后，基于Transformer网络的GPT (Generative Pre-trained Transformer) 系列模型参数量从GPT-1的亿级，
增长到到GPT-2的十亿级，再到最新的ChatGPT的千亿级，网络模型参数量随时间呈指数级增长。

然而在硬件方面，由于登纳德缩放定律（Denard Scaling）以及摩尔定律（ Moore‘s Law）的停滞，通用处理器性能提升速度明显放缓。CPU频率和晶体管
密度难以持续增长，体系结构优化空间接近上限，多核性能受到带宽和功耗的限制，通用处理器的单位性能提升的成本也越来越大。1986年至2003年通用处理器
性能每年提升约50\%，符合摩尔定律的预测，但近10年来其性能增速明显放缓\citing{ComputerArchi}。后摩尔定律时代，领域
专用架构（Domain Specific Architecture，DSA）是一种继续为上层应用提供高性能算力的解决方案，也是近年来的趋势。领域专用架构针对应用和算法
的特性定制指令和微架构，充分利用每一个晶体管，设计实现专用硬件加速器，以牺牲通用性为代价换取特定应用领域的高性能和高能效。近年来，GPU，
FPGA，ASIC等定制化硬件加速器在数据中心和边缘设备得到大规模部署，为大数据时代提供了强大的算力支撑。针对蓬勃发展的人工智能应用，研究人员
根据不同的神经网络结构定制化的提出了适应其特性的专用加速器，获得了比通用处理器更高的性能。

综上所述，人工智能应用的蓬勃发展和硬件算力的停滞不前是当下不容忽视的矛盾。这组矛盾一方面阻碍了神经网络朝着大模型，大数据，多任务
的发展趋势，另一方面也阻碍了工业界对成熟人工智能应用的落地与普及。提升算力，设计专用加速器，为算法和模型的再次进步提供强有力的支撑
已成为迫切的需求以及研究的热点。

本文选取循环神经网络作为研究对象，目的是设计专用硬件加速器以满足一类普遍应用场景的需求，达到功耗，延迟以及精度等方面性能的平衡与提升。
循环神经网络是一类用于处理序列数据的神经网络模型，广泛应用于语音识别，机器翻译和动态系统建模，在涉及时间序列相关的应用上表现出超越其他
网络模型的性能。通过向网络结构中引入反馈机制，循环神经网络一方面可以在时间维度上共享参数，做到学习不同长度样本的经验并进行泛化；另一方面，
隐藏单元作为过去信息的有损摘要可以巧妙的实现记忆和遗忘功能，从而捕获输入数据的长期依赖关系。为了获得更好的模型预测效果，循环神经网络
往往包含大量隐藏层单元，其状态的更新来自于仿射变换（矩阵向量乘法）和紧随其后的非线性变换（激活函数）。从计算的角度，这些变换是循环神经
网络中计算最密集的部分，也是循环神经网络推理最耗时的部分，计算带来的高延迟会影响交互式应用场景下的用户体验。从存储的角度，随着循环神经
网络模型规模的增长，其所需要存储的参数量呈平方增长，这将导致循环神经网络难以部署在存储资源有限的终端设备如嵌入式设备和移动设备。
从功耗的角度，IoT和端侧人工智能有严格的能源预算要求，现有的神经网络实现平台如CPU和GPU因能源消耗大不适宜在边缘场景应用。
从用户需求求角度，用户对神经网络模型预测精度，功耗和速度的需求并非一成不变，任务的到达时间可能密集亦或稀疏，任务的目的可能是识别
或者只需要判断。面对多样化的用户需求，算法和硬件需要具备弹性可伸缩的特性。

针对以上问题，现阶段的解决方案包括软件算法端减少算力需求，硬件端增加算力提供，软硬件协同设计。本文对现有的解决方案进行综合考虑，针对
具有弹性需求的应用场景提出一套从算法，软件到硬件的解决方案及设计流程。该方案采用压缩算法对循环神经网络模型进行压缩，在硬件端设计专用
加速器实现性能提升。考虑到压缩算法将原始网络压缩为精简网络的过程不需要基于训练集的反向传播（无数据集和低压缩成本特性），网络模型的
压缩过程可以在终端设备实现。这使得终端设备在无云端支持的条件下能够独立且完备的运行，并且可以根据应用场景的具体需求以合适的网络大小
运行。在硬件层面FPGA具有可重构，高并行，低功耗的特性，一方面可以快速迭代设计高效的专用硬件架构，另一方面又可以模拟资源有限的边缘应用场景，
是实现循环神经网络硬件加速器设计的理想平台。此外，软硬件协同也是循环神经网络成为完整产品的必要环节，在任务目的，功耗，资源和实现成本等
条件的约束下，本文采用合适的软硬件划分与协同以高效完成设计目标。本文无法为所有的应用和算法设计实现硬件加速方案，仅针对循环神经网络的模型压缩
和硬件加速进行研究和实践，希望能为未来的研究提供借鉴和参考。

%一会儿任务密集，需要高精度，一会儿，任务稀疏，需要低功耗，满足这类需求是必要的，因此需要自适应的变换尺寸。

%数学语言描述为输出的每一项是对先前的输出应用相同规则而产生。也正因为此，当网络需要学习长期依赖关系的信息时，基于梯度的优化会变得困难。
%研究表明，SGD在依赖关系跨度为10到20的序列上完成训练的可能性为0\citing{Bengio1994b}，具体表现为梯度消失和梯度爆炸问题\citing{Hochreiter}。 
%为解决该问题，研究人员一方面设计新的网络结构，长短期记忆网络（Long short-term Memory，LSTM），门控循环单元（Gate recurrent unit，GRU）
%和回声状态网络（Echo state network，ESN）基于此背景提出，另一方面通过增加隐藏层单元数量以增强模型记忆能力。

\section{国内外研究现状}
人工智能产业有广阔的市场前景和经济价值，但应用的研究和实际落地还存在一定距离，主要体现在模型规模越来越大和硬件资源有限这一冲突。研究人员
从神经网络压缩算法和硬件加速器设计两方面尝试给出解决方案。
\subsection{神经网络压缩算法研究现}
研究表明神经网络中存在大量的冗余性，包括结构冗余和参数冗余。神经网络架构搜索（Neural Architecture Search， NAS）是一种自动搜索并优化神经
网络架构的技术\citing{NAS_ICLR, NAS_CVPR}，其在候选神经网络结构的集合中通过某种策略搜索出最优网络结构。例如MobileNet\citing{MobileNetsV1,MoileNetsV2}，
ShuffleNet\citing{Shuffnet}，SqueezeNet\citing{}等在不损失模型准确率的条件下具有更加紧凑的模型结构。LSTM网络结构中遗忘门是最重要的结构，
输入门次之，输出门影响最小\citing{CompRNN}，GRU，MGU，S-LSTM以及JANET对LSTM模型中的门控单元进行简化，降低了网络的结构复杂度。

剪枝是应用最广泛的网络模型参数压缩方法，主要可以分为神经元剪枝和权重剪枝。前者减少神经网络中的节点数量，后者减少神经网络中的连接数量。
剪枝方法最早在上世纪90年代的论文\citing{Purning}中提出，在2015年，Han等人对剪枝方法进行改，以较小的幅度修剪权重和神经元以降低CNN
前向传播的计算复杂度\citing{WeightPurning}，并且其所提出的Train-Purne-Retrain的剪枝方法能解决剪枝后模型精度下降的问题，广泛的应用于
后续的基于剪枝的模型压缩工作中。这种对每一个权重值进行细颗粒度的剪枝方法方法尽管能够有效减少模型参数量，但却引入了对硬件
不友好计算和访存模式，即非零数据的不规则分布会带来无效的数据计算以及流水线的阻塞。ESE\citing{ESE}是一种负载均衡的剪枝方法，该方法在
剪除小权重的同时保证分配到每一个运算处理单元的计算量相当，最终相较稠密的LSTM，压缩后的模型达到了90\%的压缩比和6.2倍的加速比。C-LSTM\citing{C-LSTM}
采用结构化的剪枝方法，以二维矩阵块为剪枝粒度，剪枝后的权重消除了计算的不平衡性。文献以CNN中的Kernel，Filter，Channel为剪枝对象，同样
增加了剪枝粒度，在极大减小模型参数的同时又保持了对硬件执行友好的特性。粗颗粒剪枝具有压缩率高，数据结构规则等优势，但同时也面临着模型
准确率下降的问题，Cao\citing{Cao}提出了基于组平衡稀疏的剪枝方法，在矩阵块间保持计算平衡的基础上对每一块内部采用细颗粒的权重剪枝，提高了模型的准确率。
AutoMl\citing{AutoMl1,AutoMl2}等自动优化方法将剪枝定义为一个优化问题，能够以较高的精度自动搜索最佳的剪枝位置和比例。

\subsection{神经网络硬件加速平台研究现状}

\section{本文的主要研究内容}
本论文以时域积分方程时间步进算法的数值实现技术、后时稳定性问题以及两层平面波加速算法为重点研究内容，主要创新点与贡献如下：

\section{本论的结构安排}
本文的章节结构安排如下：
网络模型的性能。通过向网络结构中引入反馈机制，循环神经网络可以在时间维度上共享参数，。
