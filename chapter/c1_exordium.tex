\chapter{绪\hspace{6pt}论}

\section{研究背景与意义}
人人工智能（Artificial Intelligence，AI）自上世纪五十年代诞生以来，经过符号主义，联结主义，行为主义三大流派的发展，理论和技术取得了长足的进步。
近年来，基础算力的提升和大数据的兴起促使了人工智能新一轮的应用浪潮。
智能算法在语音语言，图像视频，推荐搜索，自动驾驶等领域都取得了革命性突破。
其中以深度神经网络为代表的算法相较传统算法表现出更优越的性能，在面对与日俱增的精度需求和纷繁复杂的任务场景时都能保持较好的学习和预测能力。

神经网络算法包含众多模型，其中广泛应用的网络包括前馈神经网络（feedforward neural network, FNN），循环神经网络（recurrent neural network, RNN），
卷积神经网络（convolutonal neural network, CNN），生成对抗深经网络（generative adversarial network，GAN）等。这些网络模型及其组合变体可针对性
的解决不同应用场景的任务需求。理论而言，只要数据集足够完备，模型规模足够大，神经网络可以作为万能近似器以任意精度逼近复杂函数\citing{Hornik}。
实际情况也正如此，为达到更高的精度和获得更好的模型泛化能力，神经网络朝着深层，大参数，高计算量和复杂结构的方向发展。
自2017年提出了基于注意力机制的Transformer网络结构后，基于Transformer网络的GPT (Generative Pre-trained Transformer) 系列模型参数量从GPT-1的亿级，
增长到到GPT-2的十亿级，再到最新的ChatGPT的千亿级，网络模型参数量随时间呈指数级增长。

然而在硬件方面，由于登纳德缩放定律（Denard Scaling）以及摩尔定律（ Moore‘s Law）的停滞，通用处理器性能提升速度明显放缓。CPU频率和晶体管
密度难以持续增长，体系结构优化空间接近上限，多核性能受到带宽和功耗的限制，通用处理器的单位性能提升的成本也越来越大。1986年至2003年通用处理器
性能每年提升约50\%，符合摩尔定律的预测，但近10年来其性能增速明显放缓\citing{ComputerArchi}。后摩尔定律时代，领域
专用架构（Domain Specific Architecture，DSA）是一种继续为上层应用提供高性能算力的解决方案，也是近年来的趋势。领域专用架构针对应用和算法
的特性定制指令和微架构，充分利用每一个晶体管，设计实现专用硬件加速器，以牺牲通用性为代价换取特定应用领域的高性能和高能效。近年来，GPU，
FPGA，ASIC等定制化硬件加速器在数据中心和边缘设备得到大规模部署，为大数据时代提供了强大的算力支撑。针对蓬勃发展的人工智能应用，研究人员
根据不同的神经网络结构定制化的提出了适应其特性的专用加速器，获得了比通用处理器更高的性能。

综上所述，人工智能应用的蓬勃发展和硬件算力的停滞不前是当下不容忽视的矛盾。这组矛盾一方面阻碍了神经网络朝着大模型，大数据，多任务
的发展趋势，另一方面也阻碍了工业界对成熟人工智能应用的落地与普及。提升算力，设计专用加速器，为算法和模型的再次进步提供强有力的支撑
已成为迫切的需求以及研究的热点。

本文选取循环神经网络作为研究对象，目的是设计专用硬件加速器以满足一类普遍应用场景的需求，达到功耗，延迟以及精度等方面性能的平衡与提升。
循环神经网络是一类用于处理序列数据的神经网络模型，广泛应用于语音识别，机器翻译和动态系统建模，在涉及时间序列相关的应用上表现出超越其他
网络模型的性能。通过向网络结构中引入反馈机制，循环神经网络一方面可以在时间维度上共享参数，做到学习不同长度样本的经验并进行泛化；另一方面，
隐藏单元作为过去信息的有损摘要可以巧妙的实现记忆和遗忘功能，从而捕获输入数据的长期依赖关系。为了获得更好的模型预测效果，循环神经网络
往往包含大量隐藏层单元，其状态的更新来自于仿射变换（矩阵向量乘法）和紧随其后的非线性变换（激活函数）。从计算的角度，这些变换是循环神经
网络中计算最密集的部分，也是循环神经网络推理最耗时的部分，计算带来的高延迟会影响交互式应用场景下的用户体验。从存储的角度，随着循环神经
网络模型规模的增长，其所需要存储的参数量呈平方增长，这将导致循环神经网络难以部署在存储资源有限的终端设备如嵌入式设备和移动设备。
从功耗的角度，边缘设备，IOT设备等。
从应用场景的需求角度，一会儿任务密集，需要高精度，一会儿，任务稀疏，需要低功耗，满足这类需求是必要的，因此需要自适应的变换尺寸。

现有的不足和打算解决的问题

数学语言描述为输出的每一项是对先前的输出应用相同规则而产生。也正因为此，当网络需要学习长期依赖关系的信息时，基于梯度的优化会变得困难。
研究表明，SGD在依赖关系跨度为10到20的序列上完成训练的可能性为0\citing{Bengio1994b}，具体表现为梯度消失和梯度爆炸问题\citing{Hochreiter}。 
为解决该问题，研究人员一方面设计新的网络结构，长短期记忆网络（Long short-term Memory，LSTM），门控循环单元（Gate recurrent unit，GRU）
和回声状态网络（Echo state network，ESN）基于此背景提出，另一方面通过增加隐藏层单元数量以增强模型记忆能力。

\section{国内外研究现状}
时域积分方程方法的研究始于上世纪60 年代，C.L.Bennet 等学者针对导体目
标的瞬态电磁散射问题提出了求解时域积分方程的时间步进（marching-on in-time,
MeT）算法。

\section{本文的主要研究内容}
本论文以时域积分方程时间步进算法的数值实现技术、后时稳定性问题以及两层平面波加速算法为重点研究内容，主要创新点与贡献如下：

\section{本论的结构安排}
本文的章节结构安排如下：
网络模型的性能。通过向网络结构中引入反馈机制，循环神经网络可以在时间维度上共享参数，。
