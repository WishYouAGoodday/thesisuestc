\chapter{绪\hspace{6pt}论}

\section{研究背景与意义}
人人工智能（Artificial Intelligence，AI）自上世纪五十年代诞生以来，经过符号主义，联结主义，行为主义三大流派的发展，理论和技术取得了长足的进步。
近年来，基础算力的提升和大数据的兴起促使了人工智能新一轮的应用浪潮。
智能算法在语音语言，图像视频，推荐搜索，自动驾驶等领域都取得了革命性突破。
其中以深度神经网络为代表的算法相较传统算法表现出更优越的性能，在面对与日俱增的精度需求和纷繁复杂的任务场景时都能保持较好的学习和预测能力。

神经网络算法包含众多模型，其中广泛应用的网络包括前馈神经网络（feedforward neural network, FNN），循环神经网络（recurrent neural network, RNN），
卷积神经网络（convolutonal neural network, CNN），生成对抗深经网络（generative adversarial network，GAN）等。这些网络模型及其组合变体可针对性
的解决不同应用场景的任务需求。理论而言，只要数据集足够完备，模型规模足够大，神经网络可以作为万能近似器以任意精度逼近复杂函数\citing{Hornik}。
实际情况也正如此，为达到更高的精度和获得更好的模型泛化能力，神经网络朝着深层，大参数，高计算量和复杂结构的方向发展。
自2017年提出了基于注意力机制的Transformer网络结构后，基于Transformer网络的GPT (Generative Pre-trained Transformer) 系列模型参数量从GPT-1的亿级，
增长到到GPT-2的十亿级，再到最新的ChatGPT的千亿级，网络模型参数量随时间呈指数级增长。

然而在硬件方面，由于登纳德缩放定律（Denard Scaling）以及摩尔定律（ Moore‘s Law）的停滞，通用处理器性能提升速度明显放缓。CPU频率和晶体管
密度难以持续增长，体系结构优化空间接近上限，多核性能受到带宽和功耗的限制，通用处理器的单位性能提升的成本也越来越大。后摩尔定律时代，领域
专用架构（Domain Specific Architecture，DSA）是一种继续为上层应用提供高性能算力的解决方案，也是近年来的趋势。领域专用架构针对应用和算法
的特性定制指令和微架构，充分利用每一个晶体管，设计实现专用硬件加速器。以牺牲通用性为代价换取特定应用领域的高性能和高能效。近年来，GPU，
FPGA，ASIC等定制化硬件加速器在数据中心和边缘设备得到大规模部署，为大数据时代提供了强大的算力支撑。针对蓬勃发展的人工智能应用，研究人员
针对不同的神经网络结构定制化的提出了适应其特性的专用加速器，获得了比通用处理器更高的性能。

RNN及其加速



\section{国内外研究现状}
时域积分方程方法的研究始于上世纪60 年代，C.L.Bennet 等学者针对导体目
标的瞬态电磁散射问题提出了求解时域积分方程的时间步进（marching-on in-time,
MeT）算法。

\section{本文的主要研究内容}
本论文以时域积分方程时间步进算法的数值实现技术、后时稳定性问题以及两层平面波加速算法为重点研究内容，主要创新点与贡献如下：

\section{本论的结构安排}
本文的章节结构安排如下：
